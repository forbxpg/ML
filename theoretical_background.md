# Теоретический фон: Методы регрессии OLS, WLS, IRLS

## Введение

Лабораторная работа посвящена изучению и сравнению различных методов линейной регрессии: обычных наименьших квадратов (OLS), взвешенных наименьших квадратов (WLS) и итеративно взвешенных наименьших квадратов (IRLS). Каждый метод имеет свои особенности и области применения, особенно при наличии выбросов и гетероскедастичности в данных.

## Задание 1: Обычные наименьшие квадраты (OLS)

### Теоретическое обоснование

Метод **обычных наименьших квадратов (Ordinary Least Squares - OLS)** является фундаментальным методом линейной регрессии. Основная идея - найти такие параметры модели, которые минимизируют сумму квадратов отклонений наблюдаемых значений от предсказанных.

### Математическая формулировка

Для модели линейной регрессии:

$$ y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$

где:
-  $ y_i $ - зависимая переменная
-  $ x_i$ - независимая переменная
-  $ ( \beta_0, \beta_1 )$ - параметры модели
-  $ ( \epsilon_i )$ - случайная ошибка

**Критерий минимизации OLS:**

$$min_{\beta_0, \beta_1} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2]$$

### Решение нормальных уравнений

Параметры находятся из системы нормальных уравнений:
$$
\begin{cases}
n\beta_0 + \beta_1 \sum x_i = \sum y_i \\
\beta_0 \sum x_i + \beta_1 \sum x_i^2 = \sum x_i y_i
\end{cases}
$$

### Коэффициент детерминации R²

$$ R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2} = \frac{\sum (\hat{y}_i - \bar{y})^2}{\sum (y_i - \bar{y})^2} ]$$

где:
- $( \hat{y}_i)$ - предсказанные значения
- $( \bar{y} )$ - среднее значение зависимой переменной

### Диагностика модели

Для оценки качества модели используются:
- **Стандартизованные остатки:**
$$ r_i = \frac{e_i}{\sqrt{MSE \cdot (1 - h_{ii})}} \
  где \( e_i = y_i - \hat{y}_i \) - остатки, \( h_{ii} )$$ - диагональные элементы матрицы шляп

## Задание 2: Анализ остатков OLS модели

### Стандартизованные остатки

**Формула стандартизованных остатков:**
$$ r_i = \frac{e_i}{\sqrt{MSE}} $$

где:
- $( e_i = y_i - \hat{y}_i )$ - остатки модели
- $( MSE = \frac{\sum e_i^2}{n - p} )$ - среднеквадратическая ошибка

### Робастные стандартизованные остатки

**Формула робастных стандартизованных остатков:**
$$ r_i^* = \frac{e_i}{\hat{\sigma}} $$

где \( \hat{\sigma} \) - робастная оценка масштаба ошибок (MAD-based):
$$ \hat{\sigma} = \frac{\text{MAD}}{0.6745} $$
$$ \text{MAD} = \text(median(|e_i - \text(median)(e_j)|) $$

### Проверка нормальности распределения остатков

Для проверки нормальности используются:
1. **Гистограммы распределения остатков**
2. **Q-Q графики** (квантиль-квантиль)
3. **Тесты нормальности** (Шапиро-Уилк, Колмогоров-Смирнов)

## Задание 3: Биквадратные веса и WLS модель

### Взвешенные наименьшие квадраты (WLS)

Метод **взвешенных наименьших квадратов (Weighted Least Squares - WLS)** учитывает различную надежность наблюдений путем присвоения весов.

**Критерий минимизации WLS:**
$$ \min_{\beta} \sum_{i=1}^n w_i (y_i - \mathbf{x}_i^T \boldsymbol{\beta})^2 \]$$

где \( w_i \) - веса наблюдений.

### Биквадратные веса

**Формула биквадратных весов (би-квадратная функция):**
$$ w_i = \begin{cases}
\left(1 - \left(\frac{r_i}{6 \cdot \hat{\sigma}}\right)^2\right)^2 & \text{если } |r_i| < 6 \cdot \hat{\sigma} \\
0 & \text{если } |r_i| \geq 6 \cdot \hat{\sigma}
\end{cases} \]$$

где:
- $$\( r_i \) $$- стандартизованные остатки
- $$\( \hat{\sigma} \)$$ - робастная оценка масштаба

### WLS модель

После расчета весов строится модель WLS:
$$ \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon} \]$$

с весовой матрицей $$\( \mathbf{W} = \diag(w_1, w_2, \dots, w_n) \)$$.

**Нормальные уравнения для WLS:**
$$ (\mathbf{X}^T \mathbf{W} \mathbf{X}) \boldsymbol{\beta} = \mathbf{X}^T \mathbf{W} \mathbf{y} \]$$

## Задание 4: Итеративный процесс IRLS

### Метод итеративно взвешенных наименьших квадратов (IRLS)

**IRLS (Iteratively Reweighted Least Squares)** - это итеративный метод, который сочетает идеи WLS и робастных оценок.

### Алгоритм IRLS

1. **Инициализация:** Начать с OLS модели (равные веса \( w_i = 1 \))$$
2. **Расчет остатков:** $$\( e_i^{(k)} = y_i - \hat{y}_i^{(k)} \)$$
3. **Оценка масштаба:** $$\( \hat{\sigma}^{(k)} = \frac{\text{MAD}^{(k)}}{0.6745} \)$$
4. **Расчет весов:** $$\( w_i^{(k+1)} = f\left(\frac{e_i^{(k)}}{\hat{\sigma}^{(k)}}\right) \)$$
5. **Обновление модели:** Решить WLS задачу с новыми весами
6. **Проверка сходимости:** Если изменение параметров < tolerance, остановиться
7. **Повтор:** Вернуться к шагу 2

### Критерий сходимости

$$ \max_j |\beta_j^{(k+1)} - \beta_j^{(k)}| < \epsilon \]$$

где $$\( \epsilon = 10^{-6} \)$$ (типичное значение).

### Би-квадратная весовая функция

В контексте IRLS часто используется би-квадратная функция:
$$ \psi(u) = u \left(1 - \frac{u^2}{c^2}\right)^2 \]$$

где $$\( c = 4.685 \) $$ для 95% эффективности при нормальном распределении.

## Задание 5: Анализ остатков IRLS-модели и идентификация выбросов

### Критерий идентификации выбросов

Наблюдение считается выбросом, если выполняется условие:
$$ |r_i^*| > \hat{\sigma}_{robust} \]$$

где:
- $$\( r_i^* \)$$ - робастный стандартизованный остаток
- $$\( \hat{\sigma}_{robust} \)$$ - робастное стандартное отклонение остатков

### Робастное стандартное отклонение

$$ \hat{\sigma}_{robust} = \sqrt{\frac{\sum (r_i^*)^2}{n - p}} \]$$

где \( r_i^* \) - робастные стандартизованные остатки финальной IRLS-модели.

### Графики для анализа

1. **График зависимости остатков от x** - для выявления паттернов
2. **Гистограмма остатков** - для проверки распределения
3. **Q-Q график** - для проверки нормальности

## Задание 6: Сравнение методов OLS и IRLS

### Сравниваемые характеристики

1. **Коэффициенты модели:**
   - OLS: $$\( \hat{\beta}_{OLS} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} \)$$
   - IRLS: $$\( \hat{\beta}_{IRLS} \) $$ (итоговые коэффициенты после сходимости)

2. **Коэффициенты детерминации:**
   -$$\( R^2_{OLS} \) vs \( R^2_{IRLS} \)$$ на обучающей и тестовой выборках

3. **Робастность к выбросам:**
   - OLS чувствителен к выбросам
   - IRLS устойчив к выбросам благодаря весам

### Критерии сравнения

| Характеристика | OLS | IRLS |
|---------------|-----|------|
| Чувствительность к выбросам | Высокая | Низкая |
| Эффективность при нормальных данных | Максимальная | Сниженная (95%) |
| Сложность вычислений | Низкая | Высокая |
| Интерпретируемость | Простая | Сложная |

### Рекомендации по применению

- **OLS рекомендуется:**
  - При отсутствии выбросов
  - Когда важна простота интерпретации
  - Для получения максимально эффективных оценок при нормальных данных

- **IRLS рекомендуется:**
  - При наличии потенциальных выбросов
  - Когда важна робастность результатов
  - Для данных с гетероскедастичностью

## Математические свойства методов

### Эффективность оценок

- **OLS:** Асимптотически эффективна при нормальном распределении ошибок
- **IRLS с би-квадратными весами:** Достигает 95% эффективности OLS при нормальных данных и остается эффективной при наличии выбросов

### Разложение дисперсии

**Общая дисперсия:**
$$ \sum (y_i - \bar{y})^2 = \sum (\hat{y}_i - \bar{y})^2 + \sum (y_i - \hat{y}_i)^2 \]$$

**Коэффициент детерминации:**
$$ R^2 = \frac{SSR}{SST} = \frac{\sum (\hat{y}_i - \bar{y})^2}{\sum (y_i - \bar{y})^2} \]$$

где:
- $$\( SSR \)$$ - сумма квадратов регрессии (объясненная вариация)
- $$\( SST \)$$ - общая сумма квадратов (общая вариация)

## Заключение

Каждый метод имеет свои преимущества и области применения:

1. **OLS** - простой и эффективный при идеальных условиях
2. **WLS** - учитывает гетероскедастичность
3. **IRLS** - обеспечивает робастность к выбросам через итеративное взвешивание

Правильный выбор метода зависит от характеристик данных и целей анализа. IRLS представляет собой мощный инструмент для получения надежных оценок в условиях, когда традиционные предположения линейной регрессии нарушены.

## Литература

1. Holland, Paul W., and Roy E. Welsch. "Robust regression using iteratively reweighted least-squares." Communications in Statistics-theory and Methods 6.9 (1977): 813-827.
2. Fox, John, and Sanford Weisberg. "Robust regression." An R and S-Plus companion to applied regression 91 (2002).
